{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Lokale Ausführung von LLMS mit Langchain  \n",
    "   \n",
    "Zu Beginn des Experiments war es ungewiss, ob spezifische Abhängigkeiten zu bestimmten Python-Versionen bestehen. Deshalb entschied ich mich, mit Python-Umgebungen zu arbeiten.   \n",
    "  \n",
    "Für diesen Zweck habe ich Miniconda installiert, da ich bereits einige Erfahrungen mit dieser Umgebung gesammelt habe.   \n",
    "  \n",
    "Ich habe eine Umgebung mit der Version 3.11.8 erstellt, indem ich die folgenden Befehle ausgeführt habe:   \n",
    "  \n",
    "``` bash  \n",
    "conda create -n langchain python=3.11.8   \n",
    "conda activate langchain  \n",
    "conda install langchain -c conda-forge  \n",
    "```  \n",
    "   \n",
    "Informationsquellen:  \n",
    "- [LLMS lokal ausführen](https://python.langchain.com/docs/guides/local_llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: [Ollama](https://python.langchain.com/docs/guides/local_llms#ollama)  \n",
    "   \n",
    "Ollama ist ein Modul, das als Server fungiert, um Modelle für die Interaktion über die Shell bereitzustellen.    \n",
    "Es ermöglicht das Herunterladen, Speichern und Ausführen von Modellen auf einer lokalen Maschine, was besonders nützlich ist, wenn Sie mit großen Modellen arbeiten oder eine stabile Netzwerkverbindung nicht garantiert ist.  \n",
    "   \n",
    "Zuerst muss das Ollama-Modul installiert werden. Sie können dies mit dem folgenden Befehl tun:  \n",
    "``` bash  \n",
    "conda install ollama  \n",
    "```   \n",
    "  \n",
    "Nach der Installation kann der Ollama-Server mit dem folgenden Befehl gestartet werden:  \n",
    "``` bash  \n",
    "ollama serve  \n",
    "```  \n",
    "   \n",
    "In einer anderen Shell kann nun das gewünschte Modell heruntergeladen werden. Der folgende Befehl lädt beispielsweise das Modell \"llama2:7b\" herunter:  \n",
    "``` bash  \n",
    "ollama pull llama2:7b  \n",
    "```   \n",
    "  \n",
    "Solange der `ollama server` läuft, können Sie nun direkt über die Shell mit dem heruntergeladenen Modell interagieren.    \n",
    "Der folgende Befehl startet die Interaktion mit dem Modell \"llama2:7b\":  \n",
    "``` bash  \n",
    "ollama run llama2:7b  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLaut den United Nations Department of Economic and Social Affairs gibt es aktuell (Stand: Juli 2022) etwa 7,9 billion people (7.9 × 10^9) auf der Welt.\\n\\nDie genaue Anzahl der Menschen auf der Welt kann jedoch schwanken, da die Bevölkerung ständig durch Geburten, Sterbeiten und Migration verändert wird. Zusätzlich gibt es immer wieder Unbestimmtheiten in der Datenbasis von Volkszählungen und anderen Quellen, die die genaue Anzahl der Menschen auf der Welt festlegen müssen.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2:7b\")\n",
    "\n",
    "llm.invoke(\"Wie viele Menschen gibt es auf der Welt?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: [Llama.cpp](https://python.langchain.com/docs/guides/local_llms#llama.cpp)  \n",
    "   \n",
    "Llama.cpp ist ein C++-Implementierung des Llama-Modells.   \n",
    "Es ermöglicht eine effiziente Ausführung von Llama-Modellen auf lokalen Maschinen, indem es die Hardware-Ressourcen, einschließlich CUDA-fähiger Grafikkarten, optimal nutzt.  \n",
    "   \n",
    "Zunächst muss das `llama-cpp-python` Modul installiert werden. Dieses Modul stellt die Python-Schnittstelle für die C++-Implementierung von Llama bereit.    \n",
    "Da ich eine Nvidia-Grafikkarte mit CUDA-Unterstützung besitze, erfolgt die Installation mit CUDA-Unterstützung wie folgt:  \n",
    "   \n",
    "``` bash  \n",
    "CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1   \n",
    "pip install llama-cpp-python  \n",
    "```  \n",
    "   \n",
    "Nach der Installation von `llama-cpp-python` sollte ein Modell heruntergeladen werden. \n",
    "Eine zuverlässige Quelle für passende Modelle ist Hugging Face:     \n",
    "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q4_K_S.gguf  \n",
    "   \n",
    "Quellen:  \n",
    "- [Langchain Anleitung](https://python.langchain.com/docs/integrations/llms/llamacpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/kosta/Models/llama-2-7b-chat.Q4_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.59 GiB (4.58 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3677.37 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '14'}\n",
      "Using fallback chat format: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "\n",
      "Die genaue Zahl der Menschen auf der Welt ist schwierig zu bestimmen, da die Bevölkerung ständig schnell wächst und es keine offiziellen Quellen gibt, die die Bevölkerung exakt verfolgen. Immerhin kann man sich auf basis von Schätzungen und Berechnungen eine grobe Vorstellung davon machen, wieviel Menschen es auf der Welt gibt.\n",
      "Zensus im Jahr 2019\n",
      "In einigen Ländern der Welt werden regelmäßig Zensus durchgeführt, um die Bevölkerung zu ermitteln. Zum Beispiel führte das United States Census Bureau im Jahr 2019 einen Zensus durch, bei dem die Bevölkerung der USA auf etwa 331 Millionen Menschen geschätzt wurde. Andere Länder wie China, Indien oder Indonesien führen ebenfalls regelmäßige Zensus durch, um ihre Bevölkerung zu bestimmen.\n",
      "Schätzungen der United Nations\n",
      "Die United Nations Population Division (UNPD) gibt jährlich Schätzungen zur globalen Bevölkerung heraus. Según estas estimaciones, hub aproximadamente 7,9 billion people living on Earth as of 2020. This number is based on a variety of sources, including national censuses, sample surveys, and demographic models.\n",
      "Growth rate of the population\n",
      "The population of the world is growing at a rate of approximately 1.09% per year. This means that the number of people living on Earth is increasing by about 83 million people every year. The population of the world has more than tripled since 1950, when it was around 2.5 billion people.\n",
      "Conclusion\n",
      "While it is difficult to give an exact number of people living on Earth, it is estimated that there are around 7.9 billion people worldwide as of 2020. The population of the world is growing at a rate of approximately 1.09% per year, which means that the number of people living on Earth is increasing by about 83 million people every year."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     378.58 ms\n",
      "llama_print_timings:      sample time =      57.09 ms /   441 runs   (    0.13 ms per token,  7724.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     693.52 ms /    14 tokens (   49.54 ms per token,    20.19 tokens per second)\n",
      "llama_print_timings:        eval time =   65685.93 ms /   440 runs   (  149.29 ms per token,     6.70 tokens per second)\n",
      "llama_print_timings:       total time =   67524.95 ms /   454 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'===============================\\n\\nDie genaue Zahl der Menschen auf der Welt ist schwierig zu bestimmen, da die Bevölkerung ständig schnell wächst und es keine offiziellen Quellen gibt, die die Bevölkerung exakt verfolgen. Immerhin kann man sich auf basis von Schätzungen und Berechnungen eine grobe Vorstellung davon machen, wieviel Menschen es auf der Welt gibt.\\nZensus im Jahr 2019\\nIn einigen Ländern der Welt werden regelmäßig Zensus durchgeführt, um die Bevölkerung zu ermitteln. Zum Beispiel führte das United States Census Bureau im Jahr 2019 einen Zensus durch, bei dem die Bevölkerung der USA auf etwa 331 Millionen Menschen geschätzt wurde. Andere Länder wie China, Indien oder Indonesien führen ebenfalls regelmäßige Zensus durch, um ihre Bevölkerung zu bestimmen.\\nSchätzungen der United Nations\\nDie United Nations Population Division (UNPD) gibt jährlich Schätzungen zur globalen Bevölkerung heraus. Según estas estimaciones, hub aproximadamente 7,9 billion people living on Earth as of 2020. This number is based on a variety of sources, including national censuses, sample surveys, and demographic models.\\nGrowth rate of the population\\nThe population of the world is growing at a rate of approximately 1.09% per year. This means that the number of people living on Earth is increasing by about 83 million people every year. The population of the world has more than tripled since 1950, when it was around 2.5 billion people.\\nConclusion\\nWhile it is difficult to give an exact number of people living on Earth, it is estimated that there are around 7.9 billion people worldwide as of 2020. The population of the world is growing at a rate of approximately 1.09% per year, which means that the number of people living on Earth is increasing by about 83 million people every year.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"~/Models/llama-2-7b-chat.Q4_K_S.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Wie viele Menschen gibt es auf der Welt?\n",
    "\"\"\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: [GPT4All](https://python.langchain.com/docs/guides/local_llms#gpt4all)  \n",
    "   \n",
    "GPT4All ist eine Plattform, die Zugriff auf eine Reihe von Modellen für die Generierung von Text mittels Künstlicher Intelligenz bietet.    \n",
    "Es unterstützt eine Vielzahl von Modellen, einschließlich solcher, die auf der GPT-Architektur basieren. Diese Modelle können lokal auf Ihrer Maschine ausgeführt werden, was insbesondere bei begrenzter oder instabiler Internetverbindung nützlich ist.  \n",
    "   \n",
    "Zunächst muss eines der [unterstützten Modelle](https://gpt4all.io/index.html) heruntergeladen werden.   \n",
    "Zum Beispiel kann das Modell \"mistral-7b-openorca.Q4_0.gguf\" mit dem folgenden Befehl heruntergeladen werden:  \n",
    "   \n",
    "``` bash  \n",
    "mkdir Models  \n",
    "wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf -O Models/mistral-7b-openorca.Q4_0.gguf  \n",
    "```   \n",
    "  \n",
    "Quelle:  \n",
    "- [GPT4All Anleitung](https://python.langchain.com/docs/integrations/providers/gpt4all#gpt4all-1): Diese Anleitung zeigt, wie man GPT4All in Langchain integriert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kosta/miniconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 years ago, I was in the process of moving to New York City. It was an exciting and scary time for me as it meant leaving my family and friends behind to start fresh in this big city.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10 years ago, I was in the process of moving to New York City. It was an exciting and scary time for me as it meant leaving my family and friends behind to start fresh in this big city.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# There are many CallbackHandlers supported, such as\n",
    "# from langchain.callbacks.streamlit import StreamlitCallbackHandler\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "model = GPT4All(model=\"~/Models/mistral-7b-openorca.Q4_0.gguf\", n_threads=8)\n",
    "\n",
    "# Generate text. Tokens are streamed through the callback manager.\n",
    "model(\"Once upon a time, \", callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: [llamafile](https://python.langchain.com/docs/guides/local_llms#llamafile)  \n",
    "   \n",
    "Ein Llamafile ist eine spezielle Datei, die ein Llama-Modell und den Code, der zum Ausführen des Modells benötigt wird, in einer einzigen ausführbaren Datei kapselt.    \n",
    "Dies macht es einfach, Llama-Modelle auf verschiedenen Maschinen auszuführen, ohne dass zusätzliche Software oder Bibliotheken benötigt werden.  \n",
    "   \n",
    "Die Llamafiles können direkt im Servermodus ausgeführt werden. Hier ist ein Beispiel, wie Sie ein Llamafile herunterladen und ausführen können:  \n",
    "   \n",
    "``` bash  \n",
    "wget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile -O Models/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile  \n",
    "   \n",
    "chmod +x Models/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile  # run if you're on MacOS, Linux, or BSD  \n",
    "./Models/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser  \n",
    "```  \n",
    "   \n",
    "Mit dem Befehl `chmod +x` wird die Datei ausführbar gemacht (notwendig für MacOS, Linux oder BSD). Der Befehl `--server` startet das Modell im Servermodus, und `--nobrowser` verhindert, dass ein Webbrowser automatisch geöffnet wird.\n",
    "\n",
    "```\n",
    "Quellen:\n",
    "- https://huggingface.co/models?other=llamafile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" and I'll tell you a story.\\nFor one, he is the tallest man in the world.\\nAnd three, he eats with his mouth open.\\nSure, here's another joke: How does a lion feel about being a zoo animal?\\nAnswer: It's like being stuck in traffic!\\nI like my eggs over easy.\\nI am tired of your lies.\\nWhat's the difference between a tree and an elephant?\\nA tree can't talk, but an elephant does.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.llamafile import Llamafile\n",
    "\n",
    "llm = Llamafile()\n",
    "\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
